{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Buidling Datawarehouse of US Visitor Information\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The primary objective of this data engineering capstone project is to utilize what I have learned through the program and implement the stacks and techniques in a problem solving context. Udacity provided datasets have been choosen for the project. The main dataset will be include data on immigration to the United Sates, and supplmentary datasets will include data on airport codes, United States city demographics, and temperature distribution data.\n",
    "\n",
    "#### The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "  + Identify and gather the data to be used for the project\n",
    "  + Explain what end use case is prepared for the data\n",
    "* Step 2: Explore and Assess the Data\n",
    "  + Explore the data to identify data quality issues, like missing values or duplicated data\n",
    "  + Documents steps necessasry to clean the data\n",
    "* Step 3: Define the Data Model\n",
    "  + Map out the conceptual data model\n",
    "  + List the steps necessary to pipline the data into the data model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "  + Create the data pipelines and the data model to include a data dictionary\n",
    "  + Run data quality checks to ensure the pipeline ran as expected\n",
    "* Step 5: Complete Project Write Up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Imports and installs\n",
    "from pyspark.sql import SparkSession, SQLContext, GroupedData\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from etl.extract import Extract\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Initiate Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "config(\"spark.sql.broadcastTimeout\", \"36000\").\\\n",
    "enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope\n",
    "\n",
    "In order to build a data warehouse containing US visitor information, the following information will be aggregated:\n",
    "- immigration data by cities in the States\n",
    "- temperature data in respect to the individual city\n",
    "- demographics and airports information\n",
    "\n",
    "The end product would be in the format of a master table join by the immigration and temperature data. The process will be done through a locally run Apache session. \n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "The I94 Immigration Data comes from [the US National Tourism and Trade Office website](https://travel.trade.gov/research/reports/i94/historical/2016.html).\n",
    "\n",
    "U.S. City Demographic Data comes from OpenSoft and includes data by city, state, age, population, veteran status and race.\n",
    "- i94yr = 4 digit year\n",
    "- i94mon = numeric month\n",
    "- i94cit = 3 digit code of origin city\n",
    "- i94port = 3 character code of destination USA city\n",
    "- arrdate = arrival date in the USA\n",
    "- i94mode = 1 digit travel code\n",
    "- depdate = departure date from the USA\n",
    "- i94visa = reason for immigration\n",
    "- The temperature data set comes from Kaggle. It is in csv format.\n",
    "\n",
    "I94 Immigration Data (sas_data): comes from the US National Tourism and Trade Office and includes details on incoming immigrants and their ports of entry.\n",
    "\n",
    "Airport Code Table (airport): comes from datahub.io and includes airport codes and corresponding cities.\n",
    "\n",
    "Temperature Data : includes the information on average tempearture, city, country, latitude and longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#File path for the data source\n",
    "paths = {\n",
    "    \"us_cities_demographics\" : \"us-cities-demographics.csv\",\n",
    "    \"airport_codes\" :  \"airport-codes_csv.csv\",\n",
    "    \"sas_data\" : \"sas_data/\",\n",
    "    \"temperature_data\": '/data2/GlobalLandTemperaturesByCity.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "extract = Extract(spark, paths)\n",
    "\n",
    "demographics = extract.get_cities_demographics()\n",
    "airport_codes = extract.get_airports_codes()\n",
    "immigration_data = extract.get_immigration()\n",
    "temperature_data = extract.get_temperature_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_codes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: string (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "- printSchema() function allows to see the structure and data type existing in the data frame.\n",
    "\n",
    "#### Cleaning Steps\n",
    "- data_cleaning.py existing under ETL folder contains the steps to be applied for cleaning data depending on the each dataset details.\n",
    "- Cleaning steps include, but not limited to filling null values within 0 and grouping by city and state for the columns\n",
    "- Filtering airports dataset for only the US located airports\n",
    "- Adjusting the name for the columns for natural language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Performing cleaning tasks with predefined functions\n",
    "from etl.data_cleaning import Cleaner\n",
    "\n",
    "demographics = Cleaner.get_cities_demographics(demographics)\n",
    "airport_codes = Cleaner.get_airports(airport_codes)\n",
    "immigration_data = Cleaner.get_immigration(immigration_data)\n",
    "temperature_data = Cleaner.get_temperature(temperature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "**Star Schema Dimension Tables:**\n",
    "\n",
    "- demographics_dim\n",
    "State, state_code, Total_Population, Male_Population, Female_Population, American_Indian_and_Alaska_Native, Asian, Black_or_African-American, Hispanic_or_Latino, White, Male_Population_Ratio, Female_Population_Ratio, American_Indian_and_Alaska_Native_Ratio, Asian_Ratio, Black_or_African-American_Ratio, Hispanic_or_Latino_Ratio, White_Ratio.\n",
    "\n",
    "- airports_dim\n",
    "ident, type, name, elevation_ft, continent, iso_country, iso_region, municipality, gps_code, iata_code, local_code, coordinates.\n",
    "\n",
    "- temperature_dim\n",
    "dt, AverageTemperature, AverageTemperatureUncertainty, City, Country, Latitude, Longitude, i94port\n",
    "\n",
    "**Fact Table:**\n",
    "\n",
    "- immigration_fact\n",
    "cic_id, cod_port, cod_state, visapost, matflag, dtaddto, gender, airline, admnum, fltno, visatype, cod_visa, cod_mode, cod_country_origin, cod_country_cit, year, month, bird_year, age, counter, arrival_date, departure_date, arrival_year, arrival_month, arrival_day.\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "- Transform data\n",
    "Transform demographics dataset grouping by state an calculate all the totals and ratios for every race in every state. Transform immigration dataset on order to get arrival date in different columns (year, month, day) for partitioning the dataset.\n",
    "\n",
    "- Generate Model (Star Schema):\n",
    "Create all dimensions in parquet.\n",
    "Create fact table in parquet particioned by year, month, day of th arrival date.\n",
    "Insert in fact table only items with dimension keys right. For integrity and consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from etl.transform import Transformer\n",
    "\n",
    "demographics = Transformer.demographics(demographics)\n",
    "immigration_data = Transformer.immigrants(immigration_data)\n",
    "temperature_data = Transformer.temperatue(temperature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Defined output path\n",
    "\n",
    "destination_paths = {\n",
    "    'demographics': './output/dimensions/demographics.parquet',\n",
    "    'airports': './output/dimensions/airports.parquet',\n",
    "    'temperature': './output/dimensions/temperature.parquet',\n",
    "    'facts': './output/fact/immigrations_fact.parquet'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing demographics parquet....\n",
      "writing airports parquet....\n",
      "writing temperature parquet....\n",
      "writing facts parquet....\n"
     ]
    }
   ],
   "source": [
    "# Apply models from predefined functions\n",
    "\n",
    "from etl.models import Model\n",
    "\n",
    "model = Model(spark, destination_paths)\n",
    "\n",
    "model.modelize(immigration_data, demographics, airport_codes, temperature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Apply data quality checks from predefined functions\n",
    "\n",
    "from etl.quality_check import QualityCheck\n",
    "\n",
    "checker = QualityCheck(spark, destination_paths)\n",
    "immigration_fact = checker.get_facts()\n",
    "dim_demographics, dim_airports, dim_temperature = checker.get_dimensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True True\n"
     ]
    }
   ],
   "source": [
    "# Check for value existance\n",
    "print(checker.row_count_check(dim_demographics),\n",
    "checker.row_count_check(dim_airports),\n",
    "checker.row_count_check(dim_temperature),\n",
    "checker.row_count_check(immigration_fact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for integrity\n",
    "checker.integrity_checker(immigration_fact, dim_demographics, dim_airports, dim_temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- State_code: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Total_Population: double (nullable = true)\n",
      " |-- Male_Population: double (nullable = true)\n",
      " |-- Female_Population: double (nullable = true)\n",
      " |-- American_Indian_and_Alaska_Native: long (nullable = true)\n",
      " |-- Asian: long (nullable = true)\n",
      " |-- Black_or_African-American: long (nullable = true)\n",
      " |-- Hispanic_or_Latino: long (nullable = true)\n",
      " |-- White: long (nullable = true)\n",
      " |-- Male_Population_Ratio: double (nullable = true)\n",
      " |-- Female_Population_Ratio: double (nullable = true)\n",
      " |-- American_Indian_and_Alaska_Native_Ratio: double (nullable = true)\n",
      " |-- Asian_Ratio: double (nullable = true)\n",
      " |-- Black_or_African-American_Ratio: double (nullable = true)\n",
      " |-- Hispanic_or_Latino_Ratio: double (nullable = true)\n",
      " |-- White_Ratio: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_demographics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: float (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_airports.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: string (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- cod_port: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_temperature.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cic_id: integer (nullable = true)\n",
      " |-- cod_port: string (nullable = true)\n",
      " |-- cod_state: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- cod_visa: integer (nullable = true)\n",
      " |-- cod_mode: integer (nullable = true)\n",
      " |-- cod_country_origin: integer (nullable = true)\n",
      " |-- cod_country_cit: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- bird_year: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- counter: integer (nullable = true)\n",
      " |-- arrival_date: date (nullable = true)\n",
      " |-- departure_date: date (nullable = true)\n",
      " |-- arrival_year: integer (nullable = true)\n",
      " |-- arrival_month: integer (nullable = true)\n",
      " |-- arrival_day: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_fact.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "1. The choice of tools and technologies for this project is based on the Apache Spark. Spark is an excellent tool when it comes to scale a lot of data and transform those data into useful one. I have also considered using the service provided by AWS, but for this particular dataset and task a locally run process deemed sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "2. The data can be updated for any frequency based on the need. Apache Airflow can be set up to process the data ingestion on a monthly basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "3. Scaling the pipeline wouldn't be an issue for the 100 folds increase of the amount of data. If the Spark process gets migrated to the AWS, the scalability would be only the matter of expense difference. Only the number of nodes of cluster in EMR needs to be increased to handle more data from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "4. Apache Airflow can be set up to process the data at a frequency of every day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "5. To provide the database service that can be accessed by multiple users, migrating the process to AWS and integrate S3, EMR, and Redshift service would be sufficient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
